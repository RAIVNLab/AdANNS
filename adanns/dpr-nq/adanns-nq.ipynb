{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef1bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import pyarrow as pa\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a238e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify These\n",
    "config = 'MR' # MR, RR\n",
    "index_type = 'IVFOPQ' # IP, L2, IVF, OPQ, IVFOPQ\n",
    "train_batches = False # Set to True if system has sufficient RAM\n",
    "DPR_root = '/mnt/disks/experiments/DPR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5861db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config == 'MR':\n",
    "    config_name = 'dpr-nq-d768_384_192_96_48-wiki' # MR\n",
    "else:\n",
    "    config_name = 'dpr-nq-d768-wiki' # RR-768\n",
    "    \n",
    "embeddings_file = f'{DPR_root}results/embed/{config_name}.arrow'\n",
    "emb_data = pa.ipc.open_file(pa.memory_map(embeddings_file, \"rb\")).read_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1754f2",
   "metadata": {},
   "source": [
    "## Batched Index Training (RAM-constrained)\n",
    "Learn Exact Search Indices (with IP distance) in batches over 21M passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9cb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train exact index with database and queries with embedding size 'dim' and write to disk\n",
    "def batched_train(dim):   \n",
    "    index_file = f'results/embed/exact-index/{config_name}-dim{dim}_{index_type}_batched.faiss'\n",
    "    \n",
    "    sub_index = faiss.IndexFlatIP(dim)\n",
    "    faiss_index = faiss.IndexIDMap2(sub_index)\n",
    "\n",
    "    total = 0\n",
    "    for batch in tqdm(emb_data.to_batches()):\n",
    "        batch_data = batch.to_pydict()\n",
    "        psg_ids = np.array(batch_data[\"id\"])\n",
    "\n",
    "        token_emb = np.array(batch_data[\"embedding\"], dtype=np.float32)\n",
    "        token_emb = np.ascontiguousarray(token_emb[:, :dim]) # Shape: (8192, dim)\n",
    "        faiss_index.add_with_ids(token_emb, psg_ids)\n",
    "\n",
    "        total += len(psg_ids)\n",
    "        if total % 1000 == 0:\n",
    "            logger.info(f\"indexed {total} passages\")\n",
    "\n",
    "    faiss.write_index(faiss_index, str(index_file))\n",
    "\n",
    "if(train_batches):\n",
    "    batched_train(dim=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ce428",
   "metadata": {},
   "source": [
    "## Full Training (High peak RAM Usage ~120G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee63112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21015324,)\n",
      "(21015324, 768) float32\n"
     ]
    }
   ],
   "source": [
    "if not train_batches:\n",
    "    psg_ids = np.array(emb_data['id'])\n",
    "    print(psg_ids.shape) # Passage IDs\n",
    "\n",
    "    # Takes ~5 min on our system\n",
    "    token_emb =  np.array(emb_data[\"embedding\"])\n",
    "\n",
    "    token_emb = np.hstack(token_emb)\n",
    "\n",
    "    token_emb = token_emb.reshape(21015324, -1)\n",
    "    print(token_emb.shape, token_emb.dtype) # Token Embeddings\n",
    "else:\n",
    "    raise Exception(\"Insufficient RAM to train on entire data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adcd6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M8.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 1723.532558\n",
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M16.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 1926.746812\n",
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M32.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 2302.668599\n",
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M48.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 2746.178078\n",
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M64.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 2214.350993\n",
      "Generating dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M96.faiss\n",
      "Adding DB:  (21015324, 768)\n",
      "Time to build index with d=768 : 2294.547853\n"
     ]
    }
   ],
   "source": [
    "ncell=10 # Number of IVF cells\n",
    "dims=[768] # Embedding dims to train indices over\n",
    "Ms=[8, 16, 32, 48, 64, 96] # Number of PQ sub-quantizers for IVF+OPQ\n",
    "\n",
    "for M in Ms:\n",
    "    for dim in dims:\n",
    "        if M > dim or dim%M!=0:\n",
    "            print(\"Skipping (d,M) : (%d, %d)\" %(dim, M))\n",
    "            continue\n",
    "        \n",
    "        token_emb_sliced = np.ascontiguousarray(token_emb[:, :dim])\n",
    "        faiss.normalize_L2(token_emb_sliced)\n",
    "        print(\"Adding DB: \", token_emb_sliced.shape)\n",
    "        print(f'Generating {index_type} index on config: {config_name}')\n",
    "        \n",
    "        tic = time.time()\n",
    "        # Flat L2 Index\n",
    "        if index_type == 'IP':\n",
    "            index_file = f'results/embed/IP/{config_name}-dim{dim}_IP.faiss'\n",
    "            sub_index = faiss.IndexFlatIP(dim)\n",
    "            faiss_index = faiss.IndexIDMap2(sub_index)\n",
    "\n",
    "        elif index_type == 'L2':\n",
    "            index_file = f'results/embed/L2/{config_name}-dim{dim}_L2.faiss'\n",
    "            sub_index = faiss.IndexFlatL2(dim)\n",
    "            faiss_index = faiss.IndexIDMap2(sub_index)\n",
    "\n",
    "        elif index_type == 'IVF':\n",
    "            index_file = f'results/embed/IVF/{config_name}-dim{dim}_IVF_ncell{ncell}.faiss'\n",
    "            quantizer = faiss.IndexFlatL2(dim)\n",
    "            faiss_index = faiss.IndexIVFFlat(quantizer, dim, ncell)\n",
    "            faiss_index.train(token_emb_sliced)\n",
    "            \n",
    "        elif index_type == 'OPQ':\n",
    "            index_file = f'results/embed/OPQ/{config_name}-dim{dim}_OPQ_M{M}_nbits8.faiss'\n",
    "            opq_train_db_indices = np.random.choice(token_emb_sliced.shape[0], 500000, replace=False)\n",
    "            opq_train_db = token_emb_sliced[opq_train_db_indices]\n",
    "            sub_index = faiss.index_factory(dim, f\"OPQ{M},PQ{M}x{8}\")\n",
    "            faiss_index = faiss.IndexIDMap2(sub_index)\n",
    "            faiss_index.train(opq_train_db)\n",
    "\n",
    "        elif index_type == 'IVFOPQ':\n",
    "            index_file = f'results/embed/IVFOPQ/{config_name}-dim{dim}_IVFOPQ_cell{ncell}_M{M}_nbits8.faiss'\n",
    "            sub_index = faiss.index_factory(dim, f\"OPQ{M},IVF{ncell},PQ{M}x{8}\")\n",
    "            faiss_index = faiss.IndexIDMap2(sub_index)\n",
    "            faiss_index.train(token_emb_sliced)\n",
    "        \n",
    "        faiss_index.add_with_ids(token_emb_sliced, psg_ids)\n",
    "        faiss.write_index(faiss_index, str(index_file))\n",
    "        toc = time.time()\n",
    "        \n",
    "        print(\"Generated \", index_file)\n",
    "        print(\"Time to build index with d=%d : %f\" %(dim, toc-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef008c",
   "metadata": {},
   "source": [
    "# Search (restart kernel for memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb3919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-21 20:04:27.733 | INFO     | __main__:batch_eval_dataset:94 - init Retriever from model_ckpt=ckpt/dpr-nq-d768_384_192_96_48\n",
      "2023-05-21 20:04:35.608 | INFO     | __main__:batch_eval_dataset:100 - loading index_file=results/embed/IVFOPQ/dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M16.faiss...\n",
      "2023-05-21 20:04:41.547 | INFO     | __main__:batch_eval_dataset:107 - loading passage_db_file=data/psgs-w100.lmdb...\n",
      "2023-05-21 20:04:41.758 | INFO     | __main__:batch_eval_dataset:114 - loading QA pairs from qas-data/nq-test.csv\n",
      "2023-05-21 20:04:41.803 | INFO     | __main__:batch_eval_dataset:119 - computing query embeddings...\n",
      "2023-05-21 20:04:41.804 | INFO     | __main__:batch_eval_dataset:121 - begin searching max(top_k)=200 passage for 3610 question...\n",
      "search 1668.9 queries/s, checking answers: 100%|██████████| 8/8 [10:52<00:00, 81.59s/it]\n",
      "2023-05-21 20:15:34.546 | INFO     | __main__:batch_eval_dataset:154 - #total examples: 3610\n",
      "2023-05-21 20:15:34.567 | INFO     | __main__:batch_eval_dataset:166 - precision@1:0.26204986149584486 correct_samples:946\n",
      "2023-05-21 20:15:34.567 | INFO     | __main__:batch_eval_dataset:166 - precision@5:0.46204986149584487 correct_samples:1668\n",
      "2023-05-21 20:15:34.567 | INFO     | __main__:batch_eval_dataset:166 - precision@20:0.5997229916897507 correct_samples:2165\n",
      "2023-05-21 20:15:34.567 | INFO     | __main__:batch_eval_dataset:166 - precision@100:0.714404432132964 correct_samples:2579\n",
      "2023-05-21 20:15:34.567 | INFO     | __main__:batch_eval_dataset:166 - precision@200:0.7484764542936289 correct_samples:2702\n",
      "Finished Processing!\n",
      "\n",
      "2023-05-21 20:15:40.640 | INFO     | __main__:batch_eval_dataset:94 - init Retriever from model_ckpt=ckpt/dpr-nq-d768_384_192_96_48\n",
      "2023-05-21 20:15:43.152 | INFO     | __main__:batch_eval_dataset:100 - loading index_file=results/embed/IVFOPQ/dpr-nq-d768-wiki-dim768_IVFOPQ_cell10_M32.faiss...\n",
      "2023-05-21 20:15:50.740 | INFO     | __main__:batch_eval_dataset:107 - loading passage_db_file=data/psgs-w100.lmdb...\n",
      "2023-05-21 20:15:50.815 | INFO     | __main__:batch_eval_dataset:114 - loading QA pairs from qas-data/nq-test.csv\n",
      "2023-05-21 20:15:50.851 | INFO     | __main__:batch_eval_dataset:119 - computing query embeddings...\n",
      "2023-05-21 20:15:50.852 | INFO     | __main__:batch_eval_dataset:121 - begin searching max(top_k)=200 passage for 3610 question...\n",
      "search 1811.9 queries/s, checking answers: 100%|██████████| 8/8 [06:27<00:00, 48.38s/it]\n",
      "2023-05-21 20:22:17.956 | INFO     | __main__:batch_eval_dataset:154 - #total examples: 3610\n",
      "2023-05-21 20:22:17.959 | INFO     | __main__:batch_eval_dataset:166 - precision@1:0.33407202216066484 correct_samples:1206\n",
      "2023-05-21 20:22:17.959 | INFO     | __main__:batch_eval_dataset:166 - precision@5:0.531578947368421 correct_samples:1919\n",
      "2023-05-21 20:22:17.959 | INFO     | __main__:batch_eval_dataset:166 - precision@20:0.6493074792243767 correct_samples:2344\n",
      "2023-05-21 20:22:17.959 | INFO     | __main__:batch_eval_dataset:166 - precision@100:0.7401662049861496 correct_samples:2672\n",
      "2023-05-21 20:22:17.959 | INFO     | __main__:batch_eval_dataset:166 - precision@200:0.7678670360110803 correct_samples:2772\n",
      "Finished Processing!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "split=test\n",
    "ds=nq\n",
    "\n",
    "# Change these\n",
    "d=768\n",
    "index_type=IVFOPQ\n",
    "config_name=dpr-nq-d768_384_192_96_48-wiki\n",
    "\n",
    "# Modify index_file name to the one built above\n",
    "for M in 16 32\n",
    "do\n",
    "    for d in 768\n",
    "    do\n",
    "        python rtr/cli/eval_retriever.py \\\n",
    "        --passage_db_file data/psgs-w100.lmdb \\\n",
    "        --model_ckpt ckpt/{config_name} \\\n",
    "        --index_file results/embed/${index_type}/${config_name}-dim${d}_${index_type}_cell${ncell}_M${M}.faiss \\\n",
    "        --dataset_file qas-data/${ds}-${split}.csv \\\n",
    "        --save_file results/json/reader-${config_name}-${ds}-${split}-dim${d}.jsonl \\\n",
    "        --batch_size 512 \\\n",
    "        --max_question_len 200 \\\n",
    "        --embedding_size ${d} \\\n",
    "        --metrics_file results/metrics.json \\\n",
    "        --binary False \\\n",
    "        2>&1 | tee results/logs/eval-${config_name}-${ds}-${split}-dim${d}.log\n",
    "        echo -e \"Finished Processing!\\n\"\n",
    "    done\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
