{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ade3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from os import path, makedirs\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from functools import partial\n",
    "\n",
    "from faiss.contrib.ivf_tools import add_preassigned, search_preassigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2af97",
   "metadata": {},
   "source": [
    "## AdANNS-IVF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9744b124",
   "metadata": {},
   "source": [
    "### Notation\n",
    "1. $D$ = Embedding Dimensionality for IVF construction and search\n",
    "2. $M$ = number of OPQ subquantizers. Faiss requires $D$ % $M$ == $0$. \n",
    "3. For AdANNS, D is decomposed to $D_{construct}$ and $D_{search}$\n",
    "\n",
    "### Miscellaneous Notes\n",
    "1. Rigid representations (RR) are embedded through independently trained \"fixed feature\" (FF) encoders. RR and FF are thus used interchangeably in documentation and code and are essentially equivalent.\n",
    "2. In this notebook, the AdANNS-IVF coarse quantizer uses OPQ by default for cheap distance computation, but is <u>optional</u>.\n",
    "3. AdANNS-IVF is adapted from this [Faiss Case Study](https://gist.github.com/mdouze/8c5ab227c0f7d9d7c15cf92a391dcbe5#file-demo_independent_ivf_dimension-ipynb)\n",
    "4. Optimized AdANNS-IVF (with Faiss) has a restriction that $D_{construct}\\geq D_{search}$. This is because we slice centroids learnt from $D_{construct}$ to learn PQ codebooks with $D_{search}$ (this is possible because they are MRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc82acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2048 # Max d for ResNet50\n",
    "n_cell = 1024 # number of IVF cells, default=1024 for ImageNet-1K\n",
    "\n",
    "embeddings_root = 'path/to/embeddings' # load embeddings\n",
    "adanns_root = 'path/to/adanns/indices/' # store adanns indices\n",
    "rigid_root = 'path/to/rigid/indices/' # store rigid indices\n",
    "config = 'rr' # mrl, rr\n",
    "\n",
    "if config == 'mrl':\n",
    "    config_load = 'mrl1_e0_ff2048'\n",
    "elif config == 'rr':\n",
    "    config_load = 'mrl0_e0_ff2048'\n",
    "else:\n",
    "    raise Exception(f\"Unsupported config {config}!\")\n",
    "\n",
    "use_mrl = config.upper() # MRL, RR\n",
    "\n",
    "db_npy = '1K_train_' + config_load + '-X.npy'\n",
    "query_npy = '1K_val_' + config_load + '-X.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045780fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.load(embeddings_root + db_npy)\n",
    "assert np.count_nonzero(np.isnan(xb)) == 0\n",
    "xq = np.load(embeddings_root + query_npy)\n",
    "\n",
    "query_labels = np.load(embeddings_root + \"1K_val_\" + config_load + \"-y.npy\")\n",
    "db_labels = np.load(embeddings_root + \"1K_train_\" + config_load + \"-y.npy\")\n",
    "\n",
    "print(\"loaded DB %s : %s\" % (db_npy, xb.shape))\n",
    "print(\"loaded queries %s : %s\" % (query_npy, xq.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdc1ef",
   "metadata": {},
   "source": [
    "## RR2048 OPQ Dim Reduction Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517fa2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2048)\n"
     ]
    }
   ],
   "source": [
    "db_subsampled = xb[np.random.choice(xb.shape[0], 100000, replace=False)]\n",
    "print(db_subsampled.shape)\n",
    "dim_reduce = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fe9462",
   "metadata": {},
   "source": [
    "### SVD dim reduction + OPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4829ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD projected Database:  (1281167, 128)\n",
      "SVD projected Queries:  (50000, 128)\n"
     ]
    }
   ],
   "source": [
    "def get_SVD_mat(db_subsampled, low_dim):\n",
    "    mat = faiss.PCAMatrix(db_subsampled.shape[1], low_dim)\n",
    "    mat.train(db_subsampled)\n",
    "    assert mat.is_trained\n",
    "    return mat\n",
    "\n",
    "svd_mat = get_SVD_mat(db_subsampled, dim_reduce)\n",
    "database_svd_lowdim = svd_mat.apply(xb)\n",
    "print(\"SVD projected Database: \", database_svd_lowdim.shape)\n",
    "query_svd_lowdim = svd_mat.apply(xq)\n",
    "print(\"SVD projected Queries: \", query_svd_lowdim.shape)\n",
    "\n",
    "faiss.normalize_L2(database_svd_lowdim)\n",
    "faiss.normalize_L2(query_svd_lowdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd787adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FF_D2048_SVD128_OPQ128.faiss\n",
      "Train+add time:  13411.728565454483\n",
      "[2048, 128, 128, 0.69224]\n"
     ]
    }
   ],
   "source": [
    "for M in [128]:\n",
    "    if not path.exists(f'{rigid_root}/SVD_dimreduce/{use_mrl}_D2048_SVD{dim_reduce}_OPQ{M}.faiss'):\n",
    "        print(f'Building {use_mrl}_D2048_SVD{dim_reduce}_OPQ{M}.faiss')\n",
    "        cpu_index = faiss.index_factory(dim_reduce, f'OPQ{M},PQ{M}')\n",
    "        start = time.time()\n",
    "        cpu_index.train(database_svd_lowdim)\n",
    "        cpu_index.add(database_svd_lowdim)\n",
    "        print(\"Train+add time: \", time.time() - start)\n",
    "        faiss.write_index(cpu_index, f'{rigid_root}/SVD_dimreduce/{use_mrl}_D2048_SVD{dim_reduce}_OPQ{M}.faiss')\n",
    "        \n",
    "        top1 = [xb.shape[1], dim_reduce, M]\n",
    "        _, Ind = cpu_index.search(query_svd_lowdim, 100)\n",
    "        top1.append((np.sum(db_labels[Ind[:, 0]] == query_labels)) / query_labels.shape[0])\n",
    "        print(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b042b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2048, 128, 16, 0.69088]\n"
     ]
    }
   ],
   "source": [
    "for M in [128]:\n",
    "    top1 = [xb.shape[1], dim_reduce, M]\n",
    "    svd_opq_index = faiss.read_index(f'{rigid_root}/SVD_dimreduce/{use_mrl}_D2048_SVD{dim_reduce}.faiss')\n",
    "    _, Ind = svd_opq_index.search(query_svd_lowdim, 100)\n",
    "    top1.append((np.sum(db_labels[Ind[:, 0]] == query_labels)) / query_labels.shape[0])\n",
    "    print(top1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4b00a",
   "metadata": {},
   "source": [
    "## Rigid-IVF + OPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "944b8f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping build, index exists: FF_D2048+IVF1024,OPQ16.faiss\n"
     ]
    }
   ],
   "source": [
    "# Construct Rigid Index\n",
    "\n",
    "for M in [16]:\n",
    "    for D in [2048]:\n",
    "        database = np.ascontiguousarray(xb[:,:D], dtype=np.float32)\n",
    "        faiss.normalize_L2(database)\n",
    "        \n",
    "        if M > D:\n",
    "            continue\n",
    "\n",
    "        if not path.exists(f'{rigid_root}/IVFOPQ/{use_mrl}_D{D}+IVF{n_cell},OPQ{M}.faiss'):\n",
    "            print(f'Building {use_mrl}_D{D}+IVF{n_cell},OPQ{M}.faiss')\n",
    "            start = time.time()\n",
    "\n",
    "            index = faiss.index_factory(int(D), f'IVF{n_cell},PQ{M}')\n",
    "\n",
    "            opq_index_pretrained = faiss.read_index(f'{embeddings_root}index_files/{config}/opq/1K_opq_{M}m_d{D}_nbits8.index')\n",
    "            opq = opq_index_pretrained.chain.at(0)\n",
    "\n",
    "            db = opq.apply(database)\n",
    "\n",
    "            index.train(db)\n",
    "            index.add(db)\n",
    "\n",
    "            print(\"Time: \", time.time() - start)\n",
    "            faiss.write_index(index, f'{rigid_root}/IVFOPQ/{use_mrl}_D{D}+IVF{n_cell},OPQ{M}.faiss')\n",
    "            print(f'Created IVF{n_cell},OPQ{M} index with D={D}')\n",
    "\n",
    "        else:\n",
    "            print(f'Skipping build, index exists: {use_mrl}_D{D}+IVF{n_cell},OPQ{M}.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa631e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_cell, D, M, top1]\n",
      "[1024, 2048, 8, 0.64966]\n",
      "[1024, 2048, 16, 0.6663]\n",
      "[1024, 2048, 32, 0.67724]\n",
      "[1024, 2048, 64, 0.68588]\n"
     ]
    }
   ],
   "source": [
    "# Search Rigid Index\n",
    "\n",
    "print('[n_cell, D, M, top1]')\n",
    "for D in [2048]:\n",
    "    queryset = np.ascontiguousarray(xq[:,:D], dtype=np.float32)\n",
    "    faiss.normalize_L2(queryset)\n",
    "    for M in [8, 16, 32, 64]:\n",
    "        if M > D:\n",
    "            continue\n",
    "        \n",
    "        top1 = [n_cell, D, M]\n",
    "        times = [n_cell, D, M]\n",
    "\n",
    "        index = faiss.read_index(f'{rigid_root}/IVFOPQ/{use_mrl}_D{D}+IVF{n_cell},OPQ{M}.faiss')\n",
    "       \n",
    "        opq_index_pretrained = faiss.read_index(f'{embeddings_root}index_files/{config}/opq/1K_opq_{M}m_d{D}_nbits8.index')\n",
    "        opq = opq_index_pretrained.chain.at(0)\n",
    "\n",
    "        q = opq.apply(queryset)\n",
    "\n",
    "        for nprobe in [1]:\n",
    "            start = time.time()\n",
    "            faiss.extract_index_ivf(index).nprobe = nprobe \n",
    "            Dist, Ind = index.search(q, 100)\n",
    "\n",
    "            top1.append((np.sum(db_labels[Ind[:, 0]] == query_labels)) / query_labels.shape[0])\n",
    "            times.append(time.time() - start)\n",
    "\n",
    "        print(top1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c62f6",
   "metadata": {},
   "source": [
    "## AdANNS-IVF + OPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9b187df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adanns_indices(D_search, D_construct, M, n_cell):\n",
    "    index_search = faiss.index_factory(D_search, f'OPQ{M},IVF{n_cell},PQ{M}')\n",
    "    index_construct = faiss.index_factory(D_construct, f'IVF{n_cell},Flat')\n",
    "    \n",
    "    database = np.ascontiguousarray(xb[:,:D_construct], dtype=np.float32)\n",
    "    faiss.normalize_L2(database)\n",
    "\n",
    "    # train the full-dimensional \"construct\" coarse quantizer. IVF centroid assignments are learnt with D_construct\n",
    "    if not path.exists(adanns_root+f'MRL_D{D_construct}+IVF{n_cell},PQ{M}_construct_quantizer.faiss'):\n",
    "        index_construct.train(database)\n",
    "        quantizer_construct = index_construct.quantizer\n",
    "        faiss.write_index(quantizer_construct, adanns_root+f'MRL_D{D_construct}+IVF{n_cell},PQ{M}_construct_quantizer.faiss')\n",
    "    else:\n",
    "        print(\"Index exists: \", adanns_root+f'MRL_D{D_construct}+IVF{n_cell},PQ{M}_construct_quantizer.faiss')\n",
    "\n",
    "    # prepare the \"search\" coarse quantizer. OPQ codebooks are learnt on D_search\n",
    "    if not path.exists(adanns_root+f'MRL_Dsearch{D_search}_Dconstruct{D_construct}+IVF{n_cell},OPQ{M}_search_quantizer.faiss'):\n",
    "        quantizer_construct = faiss.read_index(adanns_root+f'MRL_D{D_construct}+IVF{n_cell},PQ{M}_construct_quantizer.faiss')\n",
    "        database_search = np.ascontiguousarray(xb[:, :D_search], dtype=np.float32)\n",
    "        centroids_search = np.ascontiguousarray(quantizer_construct.reconstruct_n(0, quantizer_construct.ntotal)[:, :D_search], dtype=np.float32)\n",
    "        \n",
    "        # Apply OPQ to search DB and centroids\n",
    "        opq_index_pretrained = faiss.read_index(f'{embeddings_root}index_files/{config}/opq/1K_opq_{M}m_d{D_search}_nbits8.index')\n",
    "        print(f'Applying OPQ: 1K_opq_{M}m_d{D_search}')\n",
    "        opq = opq_index_pretrained.chain.at(0)\n",
    "        opq.apply(centroids_search)\n",
    "        opq.apply(database_search)\n",
    "        faiss.normalize_L2(database_search)\n",
    "        \n",
    "        index_ivf_search = faiss.downcast_index(faiss.extract_index_ivf(index_search))\n",
    "        index_ivf_search.quantizer.add(centroids_search)\n",
    "\n",
    "        index_ivf_search.train(database_search)\n",
    "        index_search.is_trained = True\n",
    "\n",
    "        # coarse quantization with the construct quantizer\n",
    "        _, Ic = quantizer_construct.search(database, 1) # each database vector assigned to one of num_cell centroids\n",
    "        # add operation \n",
    "        add_preassigned(index_ivf_search, database_search, Ic.ravel())\n",
    "\n",
    "        faiss.write_index(index_ivf_search, adanns_root+f'MRL_Dsearch{D_search}_Dconstruct{D_construct}+IVF{n_cell},OPQ{M}_search_quantizer.faiss')\n",
    "    else:\n",
    "        print(\"Index exists: \", adanns_root+f'MRL_Dsearch{D_search}_Dconstruct{D_construct}+IVF{n_cell},OPQ{M}_search_quantizer.faiss')\n",
    "    \n",
    "    print(f'Initialized construct quantizer D{D_construct}, search quantizer D{D_search}, M{M}, ncell{n_cell}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad99c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (M, d_small, d_big):  (64, 2048, 64)\n",
      "Skipping (M, d_small, d_big):  (64, 2048, 128)\n",
      "Skipping (M, d_small, d_big):  (64, 2048, 256)\n",
      "Skipping (M, d_small, d_big):  (64, 2048, 512)\n",
      "Skipping (M, d_small, d_big):  (64, 2048, 1024)\n",
      "Index exists:  case_study_decoupled/MRL_D2048+IVF1024,PQ64_big_quantizer.faiss\n",
      "Applying OPQ: 1K_opq_64m_d2048\n",
      "Initialized big quantizer D2048, small quantizer D2048, M64, ncell1024\n"
     ]
    }
   ],
   "source": [
    "for D_construct in [64, 128, 256, 512, 1024, 2048]:\n",
    "    for D_search in [2048]:\n",
    "        for M in [64]:\n",
    "            if M > D_search or D_search > D_construct:\n",
    "                print(\"Skipping (M, d_search, d_construct): \", (M, D_search, D_construct))\n",
    "                continue\n",
    "            create_adanns_indices(D_search, D_construct, M, n_cell=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2081b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preassigned Search using multiple cores\n",
    "\n",
    "USE_MULTITHREAD_SEARCH = True\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "thread_batch_size = 1000\n",
    "\n",
    "# Helper function to split search on multiple cores\n",
    "def multisearch_preassigned(index, queryset, Ic, batch_iter):\n",
    "    _, I = search_preassigned(index, \n",
    "                              queryset[thread_batch_size*batch_iter:thread_batch_size*(batch_iter+1)], \n",
    "                              100, # Shortlist length\n",
    "                              Ic[thread_batch_size*batch_iter:thread_batch_size*(batch_iter+1)], \n",
    "                              None)\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74680e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_adanns_indices(D_search, D_construct, n_cell, nprobes=[1]):\n",
    "    queryset = np.ascontiguousarray(xq[:,:D_construct], dtype=np.float32)\n",
    "    faiss.normalize_L2(queryset)\n",
    "    \n",
    "    queryset_small = np.ascontiguousarray(xq[:, :D_search], dtype=np.float32)\n",
    "    faiss.normalize_L2(queryset_small)\n",
    "    \n",
    "    for M in [64]:\n",
    "        top1 = [n_cell, D_construct, D_search, M]\n",
    "        times = [n_cell, D_construct, D_search, M]\n",
    "        if M > D_search or D_search > D_construct:\n",
    "                continue\n",
    "                \n",
    "        # print(f'MRL IVF{n_cell},PQ{M}: D{D_search} search with D{D_construct} coarse quantization')\n",
    "        quantizer_big = faiss.read_index(adanns_root + f'MRL_D{D_construct}+IVF{n_cell},PQ{M}_big_quantizer.faiss')\n",
    "        index_ivf_small = faiss.read_index(adanns_root + f'MRL_Dsmall{D_search}_Dbig{D_construct}+IVF{n_cell},OPQ{M}_small_quantizer.faiss')\n",
    "        \n",
    "        # disable precomputed tables, because the Dc is out of sync with the \n",
    "        # small coarse quantizer\n",
    "        index_ivf_small.use_precomputed_table = -1\n",
    "        index_ivf_small.precompute_table()\n",
    "\n",
    "        for nprobe in nprobes:\n",
    "            start = time.time()\n",
    "\n",
    "            # coarse quantization \n",
    "            _, Ic = quantizer_big.search(queryset, nprobe) # Ic: (50K, nprobe)\n",
    "\n",
    "            # actual search \n",
    "            index_ivf_small.nprobe = nprobe\n",
    "            \n",
    "            if USE_MULTITHREAD_SEARCH:\n",
    "                pool = ThreadPool(num_cores)\n",
    "                partial_func = partial(multisearch_preassigned, index=index_ivf_small, queryset=queryset_small, Ic=Ic)\n",
    "                I = pool.map(partial_func, range(queryset_small.shape[0] // thread_batch_size)) # 50K queries split to (num_batches, thread_batch_size) batches\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "                \n",
    "            else:\n",
    "                _, I = search_preassigned(index_ivf_small, queryset_small, 100, Ic, None) # I: (50K, 100)\n",
    "\n",
    "            top1.append((np.sum(db_labels[I[:, 0]] == query_labels)) / query_labels.shape[0])\n",
    "            times.append(time.time()-start)\n",
    "            \n",
    "        if (len(top1) > 4): # ignore continued cases\n",
    "            with open('adanns-faiss-top1-opq.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(top1)\n",
    "            with open('adanns-faiss-timing-opq.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(times)\n",
    "            print(top1)\n",
    "            # print(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baea8e1",
   "metadata": {},
   "source": [
    "## Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13a8b9b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_cell', 'D_big', 'D_small', 'M', '1probe', '4probe', '8probe']\n",
      "[1024, 64, 64, 64, 0.6942]\n",
      "[1024, 128, 64, 64, 0.69422]\n",
      "[1024, 128, 128, 64, 0.69584]\n",
      "[1024, 256, 64, 64, 0.69334]\n",
      "[1024, 256, 128, 64, 0.69604]\n",
      "[1024, 256, 256, 64, 0.69632]\n",
      "[1024, 512, 64, 64, 0.69418]\n",
      "[1024, 512, 128, 64, 0.69676]\n",
      "[1024, 512, 256, 64, 0.69568]\n",
      "[1024, 512, 512, 64, 0.6969]\n",
      "[1024, 1024, 64, 64, 0.69576]\n",
      "[1024, 1024, 128, 64, 0.69716]\n",
      "[1024, 1024, 256, 64, 0.69676]\n",
      "[1024, 1024, 512, 64, 0.69648]\n",
      "[1024, 1024, 1024, 64, 0.69412]\n",
      "[1024, 2048, 64, 64, 0.69444]\n",
      "[1024, 2048, 128, 64, 0.69608]\n",
      "[1024, 2048, 256, 64, 0.6973]\n",
      "[1024, 2048, 512, 64, 0.69628]\n",
      "[1024, 2048, 1024, 64, 0.69274]\n",
      "[1024, 2048, 2048, 64, 0.6899]\n"
     ]
    }
   ],
   "source": [
    "header = [\"n_cell\", \"D_construct\", \"D_search\", \"M\", \"1probe\", \"4probe\", \"8probe\"]\n",
    "print(header)\n",
    "\n",
    "with open('adanns-faiss-top1-opq.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "with open('adanns-faiss-timing-opq.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "for D_construct in [64, 128, 256, 512, 1024, 2048]:\n",
    "    for D_search in [64, 128, 256, 512, 1024, 2048]:\n",
    "            search_adanns_indices(D_search, D_construct, n_cell=1024, nprobes=[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 26 2022, 19:06:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
